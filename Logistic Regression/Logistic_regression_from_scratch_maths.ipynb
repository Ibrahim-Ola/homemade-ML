{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d6398a-c181-42e2-807b-3c24559de5ad",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Logistic Regresson from Scratch </h2> \n",
    "<h3 align=\"center\"> Author: Ibrahim O Alabi, PhDc </h3>\n",
    "\n",
    "This notebook is part of my series on Introduction to Python for Data Science. This is my way of contributing to open source knowledge. If you find this content useful, please consider leaving a **star** on this [repository](https://github.com/Ibrahim-Ola/homemade-ML.git).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d665c-7df1-450f-bfa2-5b4085124480",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Theoretical Background of Logistic Regression\n",
    "\n",
    "Consider a dataset $\\mathcal{D}_n = \\left\\lbrace (\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), \\cdots, (\\textbf{x}_n, y_n) \\right\\rbrace$ where $\\textbf{x}_i^\\top \\equiv ({x}_{i1}, {x}_{i2}, \\cdots, {x}_{ip})$ denotes the $p$-dimensional vector of characteristics of the input space $\\mathcal{X}$, and $y_i$ represents the corresponding label from the output space $\\mathcal{Y} = \\left\\lbrace 0, 1 \\right\\rbrace$. Our goal is to predict the class of any given vector $\\textbf{x}$ as accurate as possible.\n",
    "\n",
    "We start by assuming a model of the form:\n",
    "\n",
    "$$\n",
    "P(y = 1 | \\textbf{x}) = \\textbf{w}^\\top \\textbf{x} + b\n",
    "$$\n",
    "\n",
    "where $\\textbf{w} = (w_1, w_2, \\cdots, w_p)^\\top \\in \\mathbb{R}^p$ is a vector of weights and  $b \\in \\mathbb{R}$ is the bias term (scaler).\n",
    "\n",
    "### The Logistic Function\n",
    "\n",
    "In order for $\\textbf{w}^\\top \\textbf{x} + b$ to output proper probabilities, we rely on the logistic function which sqashes our linear function into a probability value. The logistic function is given as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "With the logistic function, $z \\in \\mathbb{R}$ but $\\sigma(z) \\in [0,1]$.\n",
    "\n",
    "We can then redefine our prediction equation as;\n",
    "\n",
    "$$\n",
    "P(y = 1 | \\textbf{x}) = \\sigma(\\textbf{w}^\\top \\textbf{x} + b) = \\frac{1}{1 + e^{-(\\textbf{w}^\\top \\textbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "### Training the Logistic Regression Classiifer\n",
    "\n",
    "Since the logistic regression classifier is a probabilistic learning machine, it can be optimized using the maximum likelihood approach. The likelihood of the logistic model (training data ($\\mathcal{D}_{tr}$)) is given as:\n",
    "\n",
    "$$\n",
    "L(\\textbf{w}, b| \\mathcal{D}_{tr}) = \\prod_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}}  \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)^{y_i} (1 - \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b))^{1-y_i}\n",
    "$$\n",
    "\n",
    "Where the likelihood function is the same as the Bernoulli likelihood since $y_i \\sim Ber(p) = y_i \\sim Ber(\\sigma(\\textbf{w}^\\top \\textbf{x} + b))$. Because log is a monotone function, we optimize the log-likelihood for the sake of ease.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})} = log L(\\textbf{w}, b| \\mathcal{D}_{tr}) & = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} y_i\\ log(\\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) + (1 - y_i) log(1 - \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) \\\\\n",
    "&  = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} y_i\\ log(\\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) + (1 - y_i) log(\\sigma(-(\\textbf{w}^\\top \\textbf{x}_i + b))) \n",
    "\\end{align*}\n",
    "\n",
    "Where we used the fact that $1 - \\sigma(z) = \\sigma(-z) \\qquad \\qquad \\star $\n",
    "\n",
    "### Maximizing the log-likelihood\n",
    "\n",
    "For ease of differentiation, we assume that the bias term ($b$) is inside the vector of weights ($\\textbf{w}$) and has a value of 1 in any vector $\\textbf{x}_i$.  \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\textbf{w}} \\mathcal{L (\\textbf{w}| \\mathcal{D}_{tr})} & =  \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} y_i\\textbf{x}_i\\frac{\\sigma(\\textbf{w}^\\top \\textbf{x}_i) (1 - \\sigma(\\textbf{w}^\\top \\textbf{x}_i))}{\\sigma(\\textbf{w}^\\top \\textbf{x}_i)} + (1-y_i)\\textbf{x}_i \\frac{\\sigma(-\\textbf{w}^\\top \\textbf{x}_i) (1 - \\sigma(-\\textbf{w}^\\top \\textbf{x}_i))}{\\sigma(-\\textbf{w}^\\top \\textbf{x}_i)} \\\\\n",
    "& = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} y_i\\textbf{x}_i (1 - \\sigma(\\textbf{w}^\\top \\textbf{x}_i)) + (1-y_i)\\textbf{x}_i (1 - \\sigma(-\\textbf{w}^\\top \\textbf{x}_i)) \\\\\n",
    "& = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} y_i\\textbf{x}_i (1 - \\sigma(\\textbf{w}^\\top \\textbf{x}_i)) + (1-y_i)\\textbf{x}_i (-\\sigma(\\textbf{w}^\\top \\textbf{x}_i)) \\quad [\\text{property } \\star] \\\\\n",
    "& = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} \\textbf{x}_i (y_i - y_i\\sigma(\\textbf{w}^\\top \\textbf{x}_i) - \\sigma(\\textbf{w}^\\top \\textbf{x}_i) + y_i\\sigma(\\textbf{w}^\\top \\textbf{x}_i)) \\\\\n",
    "& = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} \\textbf{x}_i (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i))\n",
    "\\end{align*}\n",
    "\n",
    "Equating the gradiet above to zero do not result in a closed form solution, as a result, we maximize using gradient ascent. It is worth noting that maximimizing the log-likelihood is equivalent to minimizing the negative log-likelihood, and in such case, we use gradient descent. Separating $b$ and $\\textbf{w}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\textbf{w}} \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})} & =  \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} \\textbf{x}_i (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) \\\\\n",
    "\\nabla_{b} \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})} & =  \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i+ b))\n",
    "\\end{align*}\n",
    "\n",
    "Note that in deriving the gradient, we used the fact that; $\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) (1 - \\sigma(z))$\n",
    "\n",
    "### Gradient Ascent\n",
    "\n",
    "The update equation for both $b$ and $\\textbf{w}$ are;\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{w}^{e+1} & = \\textbf{w}^{e} + \\eta \\left[ \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} \\textbf{x}_i (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) \\right] \\\\\n",
    "b^{e+1} & = b^{e} + \\eta \\left[ \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i+ b)) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where $e$ is the number of epochs,  ($b^0, \\textbf{w}^0$) are often randomly initialized, $\\eta$ is the learning rate.\n",
    "\n",
    "### Adding the $L^2$ Regularization\n",
    "\n",
    "Our new cost function becomes\n",
    "\n",
    "$$ \\mathcal{J}(\\textbf{w},b) = \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})} - \\frac{\\lambda}{2}\\lVert \\textbf{w} \\rVert^2 $$\n",
    "\n",
    "$$(\\textbf{w}^\\star,b^\\star)= \\underset{\\textbf{w}, b}{{\\tt argmax}}\\  \\mathcal{J}(\\textbf{w},b) $$\n",
    "\n",
    "Note that here, we penalize only the weights and not the bias term, and $\\lambda$ is the regularization parameter which controls the extent to which we penalize the weights.\n",
    "\n",
    "$$ \\mathcal{J}(\\textbf{w},b) = \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})} - \\frac{1}{2C} \\lVert \\textbf{w} \\rVert^2 $$\n",
    "where $C = \\frac{1}{\\lambda}$ is the hyperparameter controlling inverse of the model complexity (smaller values implies stronger regularization). Next, we compute the gradient,\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\textbf{w}} \\mathcal{J}(\\textbf{w},b) & = \\frac{\\partial \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})}}{\\partial \\textbf{w}} - \\frac{1}{2C} \\frac{\\partial \\lVert \\textbf{w} \\rVert^2}{\\partial \\textbf{w}} \\\\\n",
    "& = \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} \\textbf{x}_i (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) - \\frac{1}{C} \\textbf{w}\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\lVert \\textbf{w} \\rVert^2 = (\\textbf{w}^\\top\\textbf{w})^2 = \\sum_{i = 1}^n w_i^2, \\frac{\\partial \\lVert \\textbf{w} \\rVert^2}{\\partial \\textbf{w}} = 2 \\textbf{w}$\n",
    "\n",
    "The new update equations for both $b$ and $\\textbf{w}$ are,\n",
    "\\begin{align*}\n",
    "\\textbf{w}^{e+1} & = \\textbf{w}^{e} + \\eta \\left[\\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} \\textbf{x}_i (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i + b)) - \\frac{1}{C} \\textbf{w}\\right]\\\\\n",
    "b^{e+1} & = b^{e} + \\eta \\left[ \\sum_{(\\textbf{x}_i, y_i) \\in \\mathcal{D}_{tr}} (y_i - \\sigma(\\textbf{w}^\\top \\textbf{x}_i+ b)) \\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fb963-c433-4b53-a7cd-2ac68f5c7d0b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Fundamentals of Machine Learning](https://cs.mcgill.ca/~wlh/comp451/schedule.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
