{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d259dd61-01c9-48e3-9d72-031a59ec8485",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Logistic Regresson from Scratch (Code) </h2> \n",
    "<h3 align=\"center\"> Author: Ibrahim O Alabi, PhDc </h3>\n",
    "\n",
    "This notebook is part of my series on Introduction to Python for Data Science. This is my way of contributing to open source knowledge. If you find this content useful, please consider leaving a **star** on this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e927a6-f2db-45bc-9276-effe669eff83",
   "metadata": {},
   "source": [
    "## Batch Gradient Ascent Algorithm for Logistic Regression\n",
    "\n",
    "1. $\\textbf{w}^0 \\gets [0, 0, 0, ..., 0] $ (initialize the weights with 0s)\n",
    "2. $b^0 \\gets 0 $ (initialize the bias with 0)\n",
    "3. **for** epoch = 1 to Maxiter **do**\n",
    "    - Compute gradient w.r.t $\\textbf{w}$ using the training data ($\\nabla_{\\textbf{w}} \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})}$)\n",
    "    - Compute gradient w.r.t $b$ using the training data ($\\nabla_{b} \\mathcal{L (\\textbf{w}, b| \\mathcal{D}_{tr})}$)\n",
    "    - Update $\\textbf{w}$ ($\\textbf{w}^{epoch+1}$)\n",
    "    - Update $b$ ($b^{epoch+1}$)\n",
    "    \n",
    "Our code assumes that our iteration stops at the maximum iteration (Maxiter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b12935-c668-4ccc-96ff-a34ebe63f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _logistic(self, z):\n",
    "        sigmoid = 1/(1 + np.exp(-z))    ### the logistic (sigmoid) function\n",
    "        return sigmoid\n",
    "    \n",
    "    def fit(self, X, y, max_iters=1000, lr = 1e-3, C = 1):\n",
    "        \n",
    "        \"\"\"_summary_\n",
    "        \n",
    "        fit: A funtion that trains the Logistic Regression classifier using the full batch gradient descent\n",
    "\n",
    "        Args:\n",
    "            X (numpy array):      Numpy array of shape (n, p) containing the training data, where n is the sample size \n",
    "                                  and p is the number of explanatory variables.\n",
    "                             \n",
    "            y (numpy array):      Numpy array of shape (n, 1) containing the corresponding class labels.\n",
    "            \n",
    "            max_iters (integer):  Maximum number of iteration during optimization. Defaults to 1000\n",
    "            \n",
    "            lr (float):           Learning rate used in the gradient descent optimization. Defaults to 0.001\n",
    "            \n",
    "            C (float) :           Regularization hyperparameter (smaller values = stronger regularization). Defaults to 1 \n",
    "                             \n",
    "\n",
    "        Returns:\n",
    "                Numpy array of learned coefficients\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize weights\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        if self.weights is None:\n",
    "            self.weights = np.zeros((p, 1))\n",
    "            self.bias = np.zeros((1,1))\n",
    "        \n",
    "        # Maximizing the \n",
    "        for epoch in range(max_iters):\n",
    "            z = np.dot(X,self.weights) + self.bias \n",
    "            sigma = self._logistic(z) \n",
    "            dw = np.dot(X.T, (y - sigma))\n",
    "            db = np.sum((y - sigma))\n",
    "            self.weights += lr*(dw - self.weights/C)\n",
    "            self.bias += lr*db\n",
    "        self.coefs = np.concatenate((self.bias,self.weights), axis=0)\n",
    "        return self\n",
    "    \n",
    "    def predicted_prob(self, X_test):\n",
    "        \n",
    "        \"\"\"_summary_\n",
    "        \n",
    "        predicted_prob: Uses the optimal weights and bias from fit to predict probailities for the test (or train) set. \n",
    "\n",
    "        Args:\n",
    "            X_test (numpy array):   Numpy array of shape (n, p) containing the testing (or training) data, where n is the sample size \n",
    "                                    and p is the number of explanatory variables.\n",
    "                             \n",
    "        Returns:\n",
    "            y_prob:                  predicted probabilities of an instantace belonging to the class labeled 1\n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.dot(X_test,self.weights) + self.bias\n",
    "        y_prob = self._logistic(z)\n",
    "        return y_prob\n",
    "    \n",
    "    def predict(self, X_test, threshold = 0.5):\n",
    "        \n",
    "        \"\"\"_summary_\n",
    "        \n",
    "        predict: Uses the optimal weights and bias from fit to predict labels for the test set. \n",
    "                 Note that training data may also serve as testing data.\n",
    "\n",
    "        Args:\n",
    "            X_test (numpy array):   Numpy array of shape (n, p) containing the testing (or training) data, where n is the sample size \n",
    "                                    and p is the number of explanatory variables.\n",
    "                                    \n",
    "            threshold:              0 <= threshold <= 1, threshold for making decision\n",
    "                             \n",
    "        Returns:\n",
    "            prediction:             predicted class of all instances.\n",
    "        \"\"\"\n",
    "        \n",
    "        if threshold < 0 or threshold > 1:\n",
    "            raise ValueError(\"Threshold must be a probability value\") \n",
    "        else:\n",
    "            prediction = (self.predicted_prob(X_test) > threshold).astype(int)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e894af-7d0f-4439-9758-377e0b4b2d9f",
   "metadata": {},
   "source": [
    "## Let's implement our code on the iris dataset\n",
    "\n",
    "The iris dataset comes inbuilt with `sklearn.datasets`, so, let's import it from `sklearn.datasets`. In addition, we will compare our function with the LogisticRegression method from sklearn.\n",
    "\n",
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e77103ef-03b0-4d41-8816-800a58b94d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as slm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaeba0b-90f5-440e-9588-851146cee697",
   "metadata": {},
   "source": [
    "### Import dataset\n",
    "\n",
    "We will do binary classifcation (setosa against others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658b4369-35a7-4374-9d18-82fb53d53433",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = (np.where(iris.target > 0, 1, iris.target)).reshape(-1,1)\n",
    "y = 1- y  ## setosa = 1, others = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3f703-c5ab-478a-a0f1-784c912f8703",
   "metadata": {},
   "source": [
    "### Train-test Split\n",
    "\n",
    "Training data = 2/3 of the whole dataset, and testing data is 1/3 of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "855df735-55bc-4990-b09b-331dc493601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1/3,\n",
    "    shuffle=True, stratify=y,  ## to keep the class proportion in the split\n",
    "    random_state= 10  ## For reproducible results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266515f2-f373-45b4-8d43-28b6c6b6256f",
   "metadata": {},
   "source": [
    "### Standardizing\n",
    "\n",
    "Let's standardize so that all input variables have zero mean and unit standard deviation. This is going to improve the training process and prevent variables with larger scales from dominating the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ee6cad-aca8-404d-82e4-90d0e6b8619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479fe1cc-60ff-43c3-b9eb-9523a2780e75",
   "metadata": {},
   "source": [
    "* **Note**\n",
    "\n",
    "fit_transform != transform.\n",
    "\n",
    "transform uses the parameters of fit contained in fit_transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9df825-099d-4ee2-be91-040ccb6e5f18",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc637fc-a04e-4998-ab3e-a585c4dba739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10932086],\n",
       "       [-0.41956247],\n",
       "       [ 0.4754382 ],\n",
       "       [-0.6209608 ],\n",
       "       [-0.59230536]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train, max_iters=5000, lr=0.01, C = 0.05)\n",
    "model.coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b325b85-21d3-455f-8f91-a3fc14b9d284",
   "metadata": {},
   "source": [
    "### Let's Compare with sklearn's Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05292039-ddfa-49e3-8e0a-5542b1be8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=slm.LogisticRegression(random_state=0, C=0.05).fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "782c79ae-889c-4573-8e35-efde2379d78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10932086],\n",
       "       [-0.4195625 ],\n",
       "       [ 0.47543818],\n",
       "       [-0.62096099],\n",
       "       [-0.59230516]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((clf.intercept_.reshape(-1,1),clf.coef_.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2165c-ebe3-4d78-9e55-88fa4eece27f",
   "metadata": {},
   "source": [
    "Our coefficients are identical! We will continue with our own implementation of the Logistic Regression.\n",
    "\n",
    "Model: \n",
    "\n",
    "$$\n",
    "P(y_i = \\text{setosa} | \\textbf{x}_i)  = \\frac{1}{1 + e^{-(-1.109\\ -\\ 0.420x_{1i}\\ +\\ 0.475x_{2i}\\ -\\ 0.621x_{3i}\\ -\\ 0.592x_{4i})}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x_1$ = sepal length (cm)\n",
    "- $x_2$ = sepal width (cm)\n",
    "- $x_3$ = petal length (cm)\n",
    "- $x_4$ = petal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e039aa0-8834-46f8-863d-2f71479a62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 1.0\n",
      "Testing F1 Score: 1.0\n",
      "Testing precision: 1.0\n",
      "Testing recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print(f\"Testing accuracy: {metrics.accuracy_score(y_true = y_test, y_pred = y_hat)}\")\n",
    "print(f\"Testing F1 Score: {metrics.f1_score(y_true = y_test, y_pred = y_hat)}\")\n",
    "print(f\"Testing precision: {metrics.precision_score(y_true = y_test, y_pred = y_hat)}\")\n",
    "print(f\"Testing recall: {metrics.recall_score(y_true = y_test, y_pred = y_hat)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
